{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2b9855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programfilesssd2\\miniconda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\programfilesssd2\\miniconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "d:\\programfilesssd2\\miniconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pickle\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c38c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2760"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET = \"EDA\"\n",
    "with open('DICT_EDG300.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "dataX = loaded_data[TARGET][\"dataX\"]\n",
    "dataY = loaded_data[TARGET][\"dataY\"]\n",
    "len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3097babd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ae04dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入数据，训练样本2208个，验证样本552个。\n"
     ]
    }
   ],
   "source": [
    "# 十折交叉验证\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=114514)\n",
    "INDEX   = []\n",
    "KFSORT = 3\n",
    "for train_index, valid_index in kf.split(dataX):\n",
    "    INDEX.append((train_index,valid_index))\n",
    "\n",
    "# dataLoader_train = DataLoader(dataset_train, batch_size=32, shuffle=True,num_workers=4,pin_memory=True)\n",
    "# dataLoader_valid = DataLoader(dataset_valid, batch_size=32, shuffle=False,num_workers=4,pin_memory=True)\n",
    "\n",
    "dataLoader_train = DataLoader(TensorDataset(dataX[INDEX[KFSORT][0]],dataY[INDEX[KFSORT][0]]), batch_size=1280, shuffle=True,num_workers=4,pin_memory = True,prefetch_factor=8)\n",
    "dataLoader_valid = DataLoader(TensorDataset(dataX[INDEX[KFSORT][1]],dataY[INDEX[KFSORT][1]]), batch_size=1280, shuffle=False,num_workers=4,pin_memory = True,prefetch_factor=8)\n",
    "\n",
    "print(f\"载入数据，训练样本{len(dataX[train_index])}个，验证样本{len(dataY[valid_index])}个。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be8d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_weights = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "        nn.init.xavier_uniform_(self.attention_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_scores = torch.matmul(x, self.attention_weights)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        attended_x = torch.sum(x * attention_weights, dim=1)\n",
    "        return attended_x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1200, hidden_size=1024, num_layers=4, num_classes=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)  # hidden_size * 2 due to bidirectional LSTM\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # hidden_size * 2 due to bidirectional LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attended_out = self.attention(lstm_out)\n",
    "        output = self.fc(attended_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc0457",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1: 2.19401\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch2: 2.19115\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch3: 2.18958\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch4: 2.18653\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch5: 2.18318\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch6: 2.18212\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch7: 2.17893\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch8: 2.17664\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch9: 2.17432\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.34963，\t-0.18\t\n",
      "epoch10: 2.17047\t acc_t[0.388]\t lr[0.0001]\t acc_val: 0.34963，\t-0.18\t\n",
      "epoch11: 2.17183\t acc_t[0.383]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch12: 2.17041\t acc_t[0.385]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch13: 2.16968\t acc_t[0.384]\t lr[0.0001]\t acc_val: 0.34963，\t-0.18\t\n",
      "epoch14: 2.17091\t acc_t[0.388]\t lr[0.0001]\t acc_val: 0.34963，\t-0.18\t\n",
      "epoch15: 2.16774\t acc_t[0.388]\t lr[0.0001]\t acc_val: 0.34963，\t-0.18\t\n",
      "epoch16: 2.16683\t acc_t[0.388]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch17: 2.16564\t acc_t[0.388]\t lr[0.0001]\t acc_val: 0.35144，\tOK ====================\t\n",
      "epoch18: 2.16432\t acc_t[0.389]\t lr[0.0001]\t acc_val: 0.35326，\tOK ====================\t\n",
      "epoch19: 2.16310\t acc_t[0.390]\t lr[0.0001]\t acc_val: 0.34963，\t-0.36\t\n",
      "epoch20: 2.16173\t acc_t[0.389]\t lr[0.0001]\t acc_val: 0.35326，\tOK ====================\t\n",
      "epoch21: 2.16124\t acc_t[0.390]\t lr[0.0001]\t acc_val: 0.35688，\tOK ====================\t\n",
      "epoch22: 2.15957\t acc_t[0.392]\t lr[0.0001]\t acc_val: 0.36413，\tOK ====================\t\n",
      "epoch23: 2.15519\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.38043，\tOK ====================\t\n",
      "epoch24: 2.15663\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38586，\tOK ====================\t\n",
      "epoch25: 2.15585\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.38405，\t-0.18\t\n",
      "epoch26: 2.15651\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.38768，\tOK ====================\t\n",
      "epoch27: 2.15397\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.38224，\t-0.54\t\n",
      "epoch28: 2.15226\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch29: 2.15453\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38224，\t-0.54\t\n",
      "epoch30: 2.15166\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38224，\t-0.54\t\n",
      "epoch31: 2.15720\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch32: 2.15300\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.375，\t-1.26\t\n",
      "epoch33: 2.15286\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37318，\t-1.44\t\n",
      "epoch34: 2.15584\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.37681，\t-1.08\t\n",
      "epoch35: 2.15246\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.37862，\t-0.90\t\n",
      "epoch36: 2.15058\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38586，\t-0.18\t\n",
      "epoch37: 2.15414\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38586，\t-0.18\t\n",
      "epoch38: 2.14969\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.37862，\t-0.90\t\n",
      "epoch39: 2.15063\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37681，\t-1.08\t\n",
      "epoch40: 2.14958\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37681，\t-1.08\t\n",
      "epoch41: 2.14961\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.38768，\tOK ====================\t\n",
      "epoch42: 2.15186\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38586，\t-0.18\t\n",
      "epoch43: 2.14873\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.37681，\t-1.08\t\n",
      "epoch44: 2.14914\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch45: 2.14975\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38224，\t-0.54\t\n",
      "epoch46: 2.14941\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch47: 2.15003\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.37681，\t-1.08\t\n",
      "epoch48: 2.15119\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch49: 2.14880\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.38043，\t-0.72\t\n",
      "epoch50: 2.14617\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.37862，\t-0.90\t\n",
      "epoch51: 2.14648\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.38224，\t-0.54\t\n",
      "epoch52: 2.15117\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.38768，\tOK ====================\t[0.03\n",
      "epoch53: 2.14407\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38586，\t-0.18\t[0.03\n",
      "epoch54: 2.14902\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38949，\tOK ====================\t[0.03\n",
      "epoch55: 2.14766\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38224，\t-0.72\t[0.03\n",
      "epoch56: 2.14752\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-1.08\t[0.02\n",
      "epoch57: 2.14822\t acc_t[0.399]\t lr[0.0001]\t acc_val: 0.38405，\t-0.54\t[0.02\n",
      "epoch58: 2.14530\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38405，\t-0.54\t[0.02\n",
      "epoch59: 2.15200\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38586，\t-0.36\t[0.02\n",
      "epoch60: 2.14473\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\tOK ====================\t[0.02\n",
      "epoch61: 2.15041\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38405，\t-0.72\t[0.02\n",
      "epoch62: 2.14358\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.38043，\t-1.08\t[0.02\n",
      "epoch63: 2.14736\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.37681，\t-1.44\t[0.02\n",
      "epoch64: 2.14686\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.37318，\t-1.81\t[0.02\n",
      "epoch65: 2.14433\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37681，\t-1.44\t[0.02\n",
      "epoch66: 2.14460\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38405，\t-0.72\t[0.02\n",
      "epoch67: 2.14439\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38586，\t-0.54\t[0.01\n",
      "epoch68: 2.14442\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38405，\t-0.72\t[0.01\n",
      "epoch69: 2.14420\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.37681，\t-1.44\t[0.01\n",
      "epoch70: 2.14456\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.37681，\t-1.44\t[0.01\n",
      "epoch71: 2.14605\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.37862，\t-1.26\t[0.01\n",
      "epoch72: 2.14330\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-1.26\t[0.01\n",
      "epoch73: 2.14527\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38949，\t-0.18\t[0.01\n",
      "epoch74: 2.14247\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\tOK ====================\t[0.01\n",
      "epoch75: 2.14558\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.39492，\tOK ====================\t[0.01\n",
      "epoch76: 2.14393\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38405，\t-1.08\t[0.01\n",
      "epoch77: 2.14263\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.37681，\t-1.81\t[0.01\n",
      "epoch78: 2.14185\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.375，\t-1.99\t[0.01\n",
      "epoch79: 2.13853\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.39492，\tOK ====================\t[0.01\n",
      "epoch80: 2.14323\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.38586，\t-0.90\t[0.01\n",
      "epoch81: 2.14295\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38949，\t-0.54\t[0.01\n",
      "epoch82: 2.14183\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38405，\t-1.08\t[0.01\n",
      "epoch83: 2.14137\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.37862，\t-1.63\t[0.01\n",
      "epoch84: 2.14301\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-1.63\t[0.01\n",
      "epoch85: 2.14111\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38043，\t-1.44\t[0.01\n",
      "epoch86: 2.13855\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.39130，\t-0.36\t[0.01\n",
      "epoch87: 2.14077\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38768，\t-0.72\t[0.01\n",
      "epoch88: 2.14155\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38949，\t-0.54\t[0.01\n",
      "epoch89: 2.14094\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.38768，\t-0.72\t[0.01\n",
      "epoch90: 2.14077\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.38405，\t-1.08\t[0.01\n",
      "epoch91: 2.14270\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.38586，\t-0.90\t[0.01\n",
      "epoch92: 2.14057\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.38768，\t-0.72\t[0.01\n",
      "epoch93: 2.13627\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39673，\tOK ====================\t[0.01\n",
      "epoch94: 2.14110\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.01\n",
      "epoch95: 2.13516\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.01\n",
      "epoch96: 2.13573\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch97: 2.13614\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39492，\t-0.18\t[0.00\n",
      "epoch98: 2.13835\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch99: 2.13506\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch100: 2.13697\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.38949，\t-0.72\t[0.00\n",
      "epoch101: 2.13531\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch102: 2.13504\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch103: 2.13516\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39492，\t-0.18\t[0.00\n",
      "epoch104: 2.13329\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39492，\t-0.18\t[0.00\n",
      "epoch105: 2.13431\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch106: 2.13382\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38949，\t-0.72\t[0.00\n",
      "epoch107: 2.13748\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch108: 2.13728\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch109: 2.13349\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.39492，\t-0.18\t[0.00\n",
      "epoch110: 2.14093\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch111: 2.14257\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch112: 2.13937\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch113: 2.13137\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch114: 2.13647\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch115: 2.13362\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch116: 2.13258\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch117: 2.13394\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch118: 2.12769\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch119: 2.12657\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch120: 2.12966\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38949，\t-0.72\t[0.00\n",
      "epoch121: 2.12705\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch122: 2.13016\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch123: 2.12749\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch124: 2.13064\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch125: 2.12287\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch126: 2.12322\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch127: 2.12106\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch128: 2.12300\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39673，\tOK ====================\t[0.00\n",
      "epoch129: 2.11918\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.39492，\t-0.18\t[0.00\n",
      "epoch130: 2.11799\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39311，\t-0.36\t[0.00\n",
      "epoch131: 2.12027\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.39130，\t-0.54\t[0.00\n",
      "epoch132: 2.12194\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.39855，\tOK ====================\t[0.00\n",
      "epoch133: 2.11826\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39673，\t-0.18\t[0.00\n",
      "epoch134: 2.11783\t acc_t[0.423]\t lr[0.0001]\t acc_val: 0.39130，\t-0.72\t[0.00\n",
      "epoch135: 2.11742\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39492，\t-0.36\t[0.00\n",
      "epoch136: 2.11758\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39311，\t-0.54\t[0.00\n",
      "epoch137: 2.11681\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.40760，\tOK ====================\t[0.00\n",
      "epoch138: 2.11426\t acc_t[0.425]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch139: 2.11238\t acc_t[0.420]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch140: 2.11499\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39673，\t-1.08\t[0.00\n",
      "epoch141: 2.11510\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.39492，\t-1.26\t[0.00\n",
      "epoch142: 2.11529\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.39855，\t-0.90\t[0.00\n",
      "epoch143: 2.11096\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.00\n",
      "epoch144: 2.11424\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch145: 2.11513\t acc_t[0.419]\t lr[0.0001]\t acc_val: 0.39311，\t-1.44\t[0.00\n",
      "epoch146: 2.11470\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.39311，\t-1.44\t[0.00\n",
      "epoch147: 2.11120\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch148: 2.10869\t acc_t[0.426]\t lr[0.0001]\t acc_val: 0.40036，\t-0.72\t[0.00\n",
      "epoch149: 2.11261\t acc_t[0.419]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.00\n",
      "epoch150: 2.10988\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch151: 2.10999\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch152: 2.10935\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.00\n",
      "epoch153: 2.11055\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch154: 2.10796\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch155: 2.10751\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.00\n",
      "epoch156: 2.10287\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch157: 2.10641\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.40036，\t-0.72\t[0.00\n",
      "epoch158: 2.10675\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch159: 2.10721\t acc_t[0.399]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch160: 2.10249\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.01\n",
      "epoch161: 2.10775\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.01\n",
      "epoch162: 2.10375\t acc_t[0.420]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.01\n",
      "epoch163: 2.10708\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch164: 2.11123\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.01\n",
      "epoch165: 2.10405\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.01\n",
      "epoch166: 2.10734\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.01\n",
      "epoch167: 2.11021\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.01\n",
      "epoch168: 2.11358\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.01\n",
      "epoch169: 2.10567\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.01\n",
      "epoch170: 2.11006\t acc_t[0.401]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch171: 2.10668\t acc_t[0.402]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch172: 2.10653\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.01\n",
      "epoch173: 2.10422\t acc_t[0.402]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch174: 2.10291\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.01\n",
      "epoch175: 2.10260\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.01\n",
      "epoch176: 2.10278\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.01\n",
      "epoch177: 2.10701\t acc_t[0.400]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.01\n",
      "epoch178: 2.10447\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch179: 2.10106\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.01\n",
      "epoch180: 2.10115\t acc_t[0.404]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch181: 2.10283\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch182: 2.10308\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.01\n",
      "epoch183: 2.10276\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.36775，\t-3.98\t[0.01\n",
      "epoch184: 2.10106\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.36956，\t-3.80\t[0.01\n",
      "epoch185: 2.09732\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.36956，\t-3.80\t[0.01\n",
      "epoch186: 2.09686\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch187: 2.10270\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.01\n",
      "epoch188: 2.09989\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.01\n",
      "epoch189: 2.09622\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch190: 2.10480\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch191: 2.09427\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.01\n",
      "epoch192: 2.09835\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.36956，\t-3.80\t[0.01\n",
      "epoch193: 2.09865\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.01\n",
      "epoch194: 2.09965\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch195: 2.09874\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.01\n",
      "epoch196: 2.10434\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.01\n",
      "epoch197: 2.09871\t acc_t[0.400]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.01\n",
      "epoch198: 2.09980\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.01\n",
      "epoch199: 2.09784\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37137，\t-3.62\t[0.01\n",
      "epoch200: 2.09459\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.01\n",
      "epoch201: 2.09923\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.01\n",
      "epoch202: 2.09292\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.01\n",
      "epoch203: 2.09575\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch204: 2.09527\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch205: 2.10028\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch206: 2.09585\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.00\n",
      "epoch207: 2.09387\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch208: 2.09742\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch209: 2.09810\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.00\n",
      "epoch210: 2.09148\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch211: 2.09423\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch212: 2.08985\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch213: 2.09287\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch214: 2.09287\t acc_t[0.400]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch215: 2.08985\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch216: 2.09347\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch217: 2.09455\t acc_t[0.406]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch218: 2.09483\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch219: 2.09068\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch220: 2.09890\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch221: 2.09482\t acc_t[0.403]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch222: 2.09508\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch223: 2.09080\t acc_t[0.405]\t lr[0.0001]\t acc_val: 0.35869，\t-4.89\t[0.00\n",
      "epoch224: 2.09636\t acc_t[0.407]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch225: 2.09315\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch226: 2.09694\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch227: 2.09274\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch228: 2.09047\t acc_t[0.409]\t lr[0.0001]\t acc_val: 0.37137，\t-3.62\t[0.00\n",
      "epoch229: 2.08624\t acc_t[0.414]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch230: 2.09278\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch231: 2.08920\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch232: 2.08963\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch233: 2.08923\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch234: 2.08751\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch235: 2.08937\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch236: 2.08857\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch237: 2.08761\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch238: 2.08898\t acc_t[0.419]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch239: 2.08960\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch240: 2.08469\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.39673，\t-1.08\t[0.00\n",
      "epoch241: 2.08783\t acc_t[0.425]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch242: 2.09206\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch243: 2.08615\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch244: 2.08361\t acc_t[0.423]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch245: 2.08722\t acc_t[0.410]\t lr[0.0001]\t acc_val: 0.37137，\t-3.62\t[0.00\n",
      "epoch246: 2.08762\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch247: 2.08472\t acc_t[0.419]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch248: 2.08449\t acc_t[0.408]\t lr[0.0001]\t acc_val: 0.36956，\t-3.80\t[0.00\n",
      "epoch249: 2.08370\t acc_t[0.418]\t lr[0.0001]\t acc_val: 0.375，\t-3.26\t[0.00\n",
      "epoch250: 2.08576\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch251: 2.08312\t acc_t[0.411]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.00\n",
      "epoch252: 2.08314\t acc_t[0.421]\t lr[0.0001]\t acc_val: 0.39130，\t-1.63\t[0.00\n",
      "epoch253: 2.08711\t acc_t[0.412]\t lr[0.0001]\t acc_val: 0.38586，\t-2.17\t[0.00\n",
      "epoch254: 2.08366\t acc_t[0.420]\t lr[0.0001]\t acc_val: 0.39492，\t-1.26\t[0.00\n",
      "epoch255: 2.08885\t acc_t[0.428]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch256: 2.08281\t acc_t[0.417]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch257: 2.08706\t acc_t[0.427]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch258: 2.07989\t acc_t[0.424]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch259: 2.08031\t acc_t[0.420]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch260: 2.08157\t acc_t[0.421]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch261: 2.08346\t acc_t[0.427]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch262: 2.08177\t acc_t[0.428]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch263: 2.08263\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.37862，\t-2.89\t[0.00\n",
      "epoch264: 2.07744\t acc_t[0.415]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch265: 2.08792\t acc_t[0.430]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch266: 2.08794\t acc_t[0.413]\t lr[0.0001]\t acc_val: 0.39311，\t-1.44\t[0.00\n",
      "epoch267: 2.08347\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.39855，\t-0.90\t[0.00\n",
      "epoch268: 2.08424\t acc_t[0.431]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch269: 2.08280\t acc_t[0.424]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch270: 2.07868\t acc_t[0.428]\t lr[0.0001]\t acc_val: 0.39855，\t-0.90\t[0.00\n",
      "epoch271: 2.08940\t acc_t[0.429]\t lr[0.0001]\t acc_val: 0.36956，\t-3.80\t[0.00\n",
      "epoch272: 2.08431\t acc_t[0.423]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch273: 2.08554\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.39855，\t-0.90\t[0.00\n",
      "epoch274: 2.07940\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.37137，\t-3.62\t[0.00\n",
      "epoch275: 2.08624\t acc_t[0.416]\t lr[0.0001]\t acc_val: 0.37318，\t-3.44\t[0.00\n",
      "epoch276: 2.07689\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch277: 2.07969\t acc_t[0.430]\t lr[0.0001]\t acc_val: 0.37681，\t-3.07\t[0.00\n",
      "epoch278: 2.08622\t acc_t[0.429]\t lr[0.0001]\t acc_val: 0.38949，\t-1.81\t[0.00\n",
      "epoch279: 2.07835\t acc_t[0.428]\t lr[0.0001]\t acc_val: 0.38224，\t-2.53\t[0.00\n",
      "epoch280: 2.07811\t acc_t[0.431]\t lr[0.0001]\t acc_val: 0.38405，\t-2.35\t[0.00\n",
      "epoch281: 2.08232\t acc_t[0.436]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.00\n",
      "epoch282: 2.08253\t acc_t[0.433]\t lr[0.0001]\t acc_val: 0.40398，\t-0.36\t[0.00\n",
      "epoch283: 2.08015\t acc_t[0.445]\t lr[0.0001]\t acc_val: 0.38043，\t-2.71\t[0.00\n",
      "epoch284: 2.07901\t acc_t[0.427]\t lr[0.0001]\t acc_val: 0.38768，\t-1.99\t[0.00\n",
      "epoch285: 2.07922\t acc_t[0.429]\t lr[0.0001]\t acc_val: 0.40942，\tOK ====================\t[0.00\n",
      "epoch286: 2.07789\t acc_t[0.439]\t lr[0.0001]\t acc_val: 0.38586，\t-2.35\t[0.00\n",
      "epoch287: 2.07366\t acc_t[0.424]\t lr[0.0001]\t acc_val: 0.38224，\t-2.71\t[0.00\n",
      "epoch288: 2.07821\t acc_t[0.432]\t lr[0.0001]\t acc_val: 0.38586，\t-2.35\t[0.00\n",
      "epoch289: 2.08002\t acc_t[0.433]\t lr[0.0001]\t acc_val: 0.39492，\t-1.44\t[0.00\n",
      "epoch290: 2.07948\t acc_t[0.435]\t lr[0.0001]\t acc_val: 0.40036，\t-0.90\t[0.00\n",
      "epoch291: 2.07494\t acc_t[0.437]\t lr[0.0001]\t acc_val: 0.39855，\t-1.08\t[0.00\n",
      "epoch292: 2.08031\t acc_t[0.433]\t lr[0.0001]\t acc_val: 0.39492，\t-1.44\t[0.00\n",
      "epoch293: 2.08058\t acc_t[0.434]\t lr[0.0001]\t acc_val: 0.38768，\t-2.17\t[0.00\n",
      "epoch294: 2.07464\t acc_t[0.439]\t lr[0.0001]\t acc_val: 0.40579，\t-0.36\t[0.00\n",
      "epoch295: 2.07535\t acc_t[0.442]\t lr[0.0001]\t acc_val: 0.40217，\t-0.72\t[0.00\n",
      "epoch296: 2.07836\t acc_t[0.430]\t lr[0.0001]\t "
     ]
    }
   ],
   "source": [
    "modelName = \"LSTM\"\n",
    "target = TARGET+\"300\"\n",
    "model = LSTMModel()\n",
    "# model.load_state_dict(torch.load('./OUTPUT/model/model_LSTM_EDA300_5(1).pth'), strict=True)\n",
    "    \n",
    "(lossHistory,accHistory,trainAccHis) = TRAIN_CNN(model,modelName =modelName,target=target,KFSORT=KFSORT,\n",
    "                                                 dataLoader_train=dataLoader_train,dataLoader_valid=dataLoader_valid,\n",
    "                                                 dataCount = len(dataX[train_index]),\n",
    "                                                 epochs=2000,lr=0.0001,\n",
    "                                                 scheduler_step_size=500,scheduler_gamma=0.99)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b9c1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3361b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:582\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f74d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 43\n",
    "# 60 42\n",
    "epochs=500\n",
    "\n",
    "\n",
    "x = range(0,epochs) # x轴坐标值113  0.00009 -> 81\n",
    "plt.plot(trainAccHis,c = 'r') # 参数c为color简写，表示颜色,r为red即红色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede050f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = range(0,epochs) # x轴坐标值\n",
    "plt.plot(lossHistory,c = 'r') # 参数c为color简写，表示颜色,r为red即红色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34754162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = range(0,epochs) # x轴坐标值\n",
    "plt.plot(accHistory,c = 'r') # 参数c为color简写，表示颜色,r为red即红色\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./OUTPUT/loss/loss_\"+modelName+\"_\"+TARGET+\"300_5(\"+str(KFSORT+1)+\").pkl\", 'wb') as file:\n",
    "    pickle.dump(dict({\"loss\":lossHistory,\"acc\":accHistory,\"trainAccHis\":trainAccHis}), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP\n",
    "model = MODEL_CNN_EDA(input_channel=1)\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 500\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #0.0008\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.97)\n",
    "\n",
    "lossHistory = []\n",
    "accHistory = []\n",
    "trainAccHis = []\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=1200,filename=\"CNN_\"+TARGET+\"300_5(\"+str(KFSORT+1)+\").pth\")\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    model.train()\n",
    "    LOSS = 0\n",
    "    ACC = 0\n",
    "    for batch_data, batch_targets in dataLoader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data.to(device))\n",
    "        loss = criterion(outputs, batch_targets.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        LOSS += loss.detach().cpu().numpy()\n",
    "        ACC += (torch.max(outputs,dim=1)[1] == batch_targets.to(device)).sum()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    lossHistory.append(LOSS)\n",
    "    \n",
    "    print(f\"epoch{epoch+1}: {str(LOSS)[0:7]}\",end=\"\\t \")\n",
    "    cal = (ACC/dataCount).cpu()\n",
    "    print(f\"acc_t[{str(np.array(cal))[0:5]}]\",end=\"\\t \")\n",
    "    trainAccHis.append(cal)\n",
    "    print(\"lr[\"+str(optimizer.param_groups[0]['lr'])+\"]\",end=\"\\t \")\n",
    "    \n",
    "#     for p in model.parameters():\n",
    "#         print(p.grad.data)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_targets = []\n",
    "        for batch_data, batch_targets in dataLoader_valid:\n",
    "            outputs = model(batch_data.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_outputs.append(predicted.cpu().numpy())\n",
    "            all_targets.append(batch_targets.cpu().numpy())\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "        accuracy = accuracy_score(all_targets, all_outputs)\n",
    "        accHistory.append(accuracy)\n",
    "#         accAverHis.append(np.mean(np.array(accHistory)))\n",
    "        print(f\"acc_val: {str(accuracy)[0:7]}，\",end=\"\\t\")\n",
    "        print('OK '+'='*20 if(np.max(np.array(accHistory)) == accuracy) else str(100*(accuracy - np.max(np.array(accHistory))))[0:5],end=\"\\t\")\n",
    "    \n",
    "    \n",
    "    if(epoch>50):\n",
    "        print(f\"[{ str(np.abs(np.mean(lossHistory)-np.mean(lossHistory[0:epoch-50])))[0:4] }\")\n",
    "#         if(np.abs(np.mean(lossHistory)-np.mean(lossHistory[0:epoch-50]))<0.05):\n",
    "#             if(np.abs(100*(accuracy - np.max(np.array(accHistory))))<5):\n",
    "#                 print(\"已提前结束。\")\n",
    "#                 break\n",
    "\n",
    "                \n",
    "    else:\n",
    "        print(\"\")\n",
    "                    \n",
    "    early_stopping(accuracy, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
